<h3
id="btrfs-enhanced-chunk-allocation-with-device-roles-groups-and-priority">Btrfs:
Enhanced Chunk Allocation with Device Roles, Groups, and Priority</h3>
<h4 id="introduction">Introduction</h4>
<p>The current chunk allocator in Btrfs focuses on free space only. It
works, but it doesn’t give us enough control in heterogeneous setups
especially when we want to manage performance, fault tolerance, or how
space is consumed across devices.</p>
<p>This document lays out a proposal for a more flexible chunk
allocation system. The idea is to let users guide chunk placement using
simple policies that map better to real-world hardware setups. We reuse
existing fields in <code>btrfs_dev_item</code> and introduce new
semantics around roles, priorities, and device groups, with minimal
impact on the on-disk format.</p>
<h4 id="whats-broken-today">What’s Broken Today</h4>
<p>The allocator does the simplest thing: pick the device with the most
free space. That’s fine in uniform setups, but creates problems in more
complex ones:</p>
<ul>
<li><p><strong>Performance bottlenecks in mixed devices</strong>
Metadata doesn’t always land on the faster devices. In an SSD + HDD
setup, we want metadata on the SSD and data on the HDD. Today, we can’t
enforce that.</p></li>
<li><p><strong>Awkward space usage in SINGLE/DUP</strong> If you’re
growing or shrinking a volume, the allocator might spread data across
devices even when that’s not what you want. There’s no easy way to fill
up one disk and move on to the next, or to balance across disks of
uneven sizes.</p></li>
<li><p><strong>No fault domain awareness</strong> RAID1 and friends
protect against device failure, but not transport failures (e.g., a bad
HBA). If both mirrors land on disks behind the same controller, you
still lose everything.</p></li>
</ul>
<h4 id="why-not-just-detect-performance">Why Not Just Detect
Performance?</h4>
<p>Some might ask: why not auto-detect which device is faster?</p>
<p>The short answer is: it doesn’t work well.</p>
<ul>
<li><p>Latency data (iostat etc.) is noisy and not reliable for
permanent decisions.</p></li>
<li><p>Vendor performance hints are rare and usually require out-of-band
knowledge.</p></li>
<li><p>In virtual setups, “rotational” might mean anything.</p></li>
</ul>
<p>Instead, this design proposes letting users set device priorities
explicitly. External tools can help determine these priorities at
<code>mkfs</code> time or later using <code>btrfs properties</code> to
make it automatic, but the key idea is: noisy iostat or guessing the
device performance by type must not be used to make permanent decisions,
let the admin/tools decide.</p>
<h4 id="device-allocation-priority-and-roles">Device Allocation Priority
and Roles</h4>
<p>We introduce allocation priorities. An allocation priority (1 - 255)
determines the order in which devices are selected for chunk allocation.
Multiple devices can share the same priority if the admin considers them
to have similar performance. Lower values mean a device is more suitable
for allocation; mid-range values indicate it’s less preferred; and high
values indicate the device must not be used at all.</p>
<p>At allocation time, devices are sorted by their priority, possibly
adjusted by other modes. The sorting can be ascending or descending
based on the chunk type. Devices are then selected from the top of the
list.</p>
<p>For this purpose, we’re reusing bits in
<code>btrfs_dev_item::type</code> to track allocation preferences.
Here’s the layout:</p>
<pre><code>    struct btrfs_dev_item {
        ...
        union {
            __le64 type;
            struct {
                __le8 reserved[6];
                __le8 alloc_mode;     /* bits 8 - 15: mode */
                __le8 alloc_priority; /* bits 0 - 07: priority */
            };
        };
        __le32 dev_group;  /* FT device groups */
        ...
    };</code></pre>
<ul>
<li><code>alloc_mode</code>: defines how this device participates in
allocation (bitmask)
<ul>
<li><code>FREE_SPACE</code>: legacy</li>
<li><code>ROLE</code>: honor role bits</li>
<li><code>PRIORITY</code>: use raw alloc_priority</li>
<li><code>LINEAR</code>: sequential allocation</li>
<li><code>ROUND-ROBIN</code>: pick the next device</li>
<li><code>FT_GROUP</code>: use dev_group for fault domains</li>
</ul></li>
<li><code>alloc_priority</code>: 1 - 255; lower means higher
priority</li>
</ul>
<h5 id="roles-controlling-what-goes-where">Roles: Controlling What Goes
Where</h5>
<p>Device roles - such as <code>metadata_only</code>,
<code>metadata</code>, <code>data_only</code>, <code>data</code>,
<code>both</code>, or <code>none</code> - describe how a device should
be used and come with predefined allocation priorities. These guide
allocation. The allocator walks through the roles in order of preference
and picks the one with the most free space.</p>
<ul>
<li>metadata_only : only metadata chunks go here</li>
<li>metadata : metadata</li>
<li>preferred none || any : no preference (default)</li>
<li>data : data preferred</li>
<li>data_only : only data chunks go here</li>
</ul>
<p>Chunks are allocated according to a role-based order.</p>
<p>Ascending sort order is used for metadata chunks, prioritizing
devices in the order: <code>metadata_only</code> -&gt;
<code>metadata</code> -&gt; <code>none</code> -&gt;
<code>data</code>.<br />
Descending sort order is used for data chunks, prioritizing devices in
the order: <code>data_only</code> -&gt; <code>data</code> -&gt;
<code>none</code> -&gt; <code>metadata</code>.</p>
<p>These roles must be manually assigned, either during
<code>mkfs</code> or later using <code>btrfs property</code> at runtime.
Alternatively, an external tool that ranks devices by performance and
passes the sorted list to <code>mkfs</code> or
<code>btrfs property</code>, giving admins full control.</p>
<h4 id="device-groups-fault-domain-awareness">Device Groups: Fault
Domain Awareness</h4>
<p>To help with fault tolerance beyond a single Fault Tolerance
connection failure, we introduce <em>device groups</em> via the
<code>dev_item::dev_group</code> field.</p>
<pre><code>  +-------------------------+
  |           Host          |
  +--+------------------+---+
     |                  |
 Fault Tolerance   Fault Tolerance
 Connection 1      Connection 2
      |                  |
+-----+------+      +----+-------+
|  Cloud     |      |  Cloud     |
|  Storage   |      |  Storage   |
| Device A   |      | Device C   |
| (Low Lat)  |      | (Low Lat)  |
| Device B   |      | Device D   |
|(Normal Lat)|      |(Normal Lat)|
+------------+      +------------+</code></pre>
<p>Each device can be assigned a group ID (1 - 4). The allocator ensures
mirrored chunks (RAID1/RAID10/RAID1C*) are placed on devices in
different groups.</p>
<p>This avoids putting both mirrors on disks behind the same
connection.</p>
<p>Usage:</p>
<pre><code>mkfs.btrfs -draid1 -mraid1 sda:ft=1 sdb:ft=1 sdc:ft=2 sdd:ft=2</code></pre>
<p>The <code>dev_item::dev_group</code> field is used to track this on
disk.</p>
<h4 id="more-control-over-space-usage">More Control Over Space
Usage</h4>
<p>We add new allocation strategies for SINGLE and DUP profiles:</p>
<ul>
<li><p><strong>Linear allocation (by devid)</strong> Fill the lowest
devid until full, then move to the next. Simple, predictable. Helpful
for adding temporary space.</p></li>
<li><p><strong>Round-robin (by_devid)</strong> Distribute chunks evenly
across all devices.</p></li>
<li><p><strong>Heterogeneous performance-aware</strong> Use role-based
preference: metadata to fast devices, data to slow ones. Falls back to
free space when needed.</p></li>
</ul>
<p>These modes let users manage their volumes more deliberately.</p>
<h4 id="device-id-considerations">Device ID Considerations</h4>
<p>Linear and Round-robin allocation depends on devid order. This is
mostly assigned by the mkfs and kernel in the order devices are added.
So it’s not configurable, but in practice it’s stable: devices retain
their devid on replace, and new devices get the highest unused ID. We
accept this limitation because the benefits are still worth it.</p>
<h4 id="putting-it-all-together">Putting It All Together</h4>
<p>Here’s what we gain from this design: * <strong>Better
performance</strong> Metadata lands on SSDs or NVMe, where it belongs.
Data lands on slower devices if needed.</p>
<ul>
<li><p><strong>Improved fault tolerance</strong> No more losing both
mirrors to a bad cable or failed HBA or Network.</p></li>
<li><p><strong>More control</strong> Admins can decide how data is
spread - or not spread - across disks.</p></li>
<li><p><strong>Minimal on-disk changes</strong> We reuse fields already
present in <code>btrfs_dev_item</code>.</p></li>
<li><p><strong>No magic auto-detection</strong> We avoid estimating
device performance from noisy runtime statistics, but roles can be
assigned manually or automatically using external, flexible
tools.</p></li>
</ul>
<h4 id="future-directions">Future Directions</h4>
<p>While this proposal avoids dynamic metrics, it keeps the option open.
If reliable performance hints become available - say, from a vendor API
or BPF tool - they can populate <code>seek_speed</code> and
<code>bandwidth</code> fields in the future.</p>
<h4 id="conclusion">Conclusion</h4>
<p>This design gives Btrfs users more control while keeping what already
works. It helps solve real-world problems like poor metadata
performance, bad fault domain placement, and lack of flexibility in
space usage. Admins get tools to guide allocation at mkfs time. And we
get a system that works better in the kinds of setups people actually
run.</p>
